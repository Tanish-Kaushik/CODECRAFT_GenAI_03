# -*- coding: utf-8 -*-
"""Text Generation with Markov Chains

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ptY1Y7RKR6gRp47-FfEqVSZWFUE5Ohkp
"""

import random
import re
from collections import defaultdict, Counter

class MarkovChain:
    def __init__(self, order=2, mode='word'):
        """
        Initialize Markov Chain

        Args:
            order (int): Number of previous tokens to consider for prediction
            mode (str): 'word' for word-level or 'char' for character-level
        """
        self.order = order
        self.mode = mode
        self.model = defaultdict(Counter)
        self.start_tokens = []

    def preprocess_text(self, text):
        """Preprocess text based on mode (word or character)"""
        if self.mode == 'word':
            # Split into words, keeping basic punctuation
            tokens = re.findall(r"[\w']+|[.!?]", text)
            return [token.lower() for token in tokens]
        else:
            # Character level - keep everything
            return list(text)

    def train(self, text):
        """Train the Markov model on the provided text"""
        tokens = self.preprocess_text(text)

        if len(tokens) < self.order:
            raise ValueError(f"Text is too short for order {self.order}")

        # Build the model
        for i in range(len(tokens) - self.order):
            # Current state is a tuple of order tokens
            state = tuple(tokens[i:i + self.order])
            next_token = tokens[i + self.order]

            # Add to transition counts
            self.model[state][next_token] += 1

            # Track possible starting states (those that start sentences)
            if i == 0 or tokens[i-1] in '.!?':
                self.start_tokens.append(state)

    def generate(self, length=100, start_with=None):
        """Generate text of specified length"""
        if not self.model:
            raise ValueError("Model must be trained before generation")

        # Choose starting state
        if start_with:
            # Use provided starting text
            start_tokens = self.preprocess_text(start_with)
            if len(start_tokens) < self.order:
                # Pad if necessary
                start_tokens.extend([''] * (self.order - len(start_tokens)))
            current_state = tuple(start_tokens[:self.order])
        else:
            # Choose random starting state
            current_state = random.choice(self.start_tokens)

        # Initialize result with starting state
        if self.mode == 'word':
            result = list(current_state)
        else:
            result = list(''.join(current_state))

        # Generate tokens
        for _ in range(length - self.order):
            # Get possible next tokens
            if current_state in self.model:
                next_tokens = list(self.model[current_state].keys())
                weights = list(self.model[current_state].values())

                # Choose next token based on probabilities
                next_token = random.choices(next_tokens, weights=weights)[0]

                # Add to result
                result.append(next_token)

                # Update state
                if self.mode == 'word':
                    current_state = tuple(result[-self.order:])
                else:
                    # For character mode, we need to rebuild the state
                    current_text = ''.join(result)
                    current_state = tuple(current_text[-self.order:])
            else:
                # If we hit an unknown state, start fresh
                current_state = random.choice(self.start_tokens)
                if self.mode == 'word':
                    result.extend(current_state)
                else:
                    result.extend(''.join(current_state))

        # Format output
        if self.mode == 'word':
            # Join words with spaces, handle punctuation
            output = ' '.join(result)
            # Fix spacing around punctuation
            output = re.sub(r'\s+([.,!?])', r'\1', output)
            return output
        else:
            return ''.join(result)

    def get_probability(self, state, next_token):
        """Get the probability of a token following a given state"""
        if state not in self.model or next_token not in self.model[state]:
            return 0.0

        total = sum(self.model[state].values())
        return self.model[state][next_token] / total

def preprocess_text(self, text):
        """Preprocess text based on mode (word or character)"""
        if self.mode == 'word':
            # Split into words, keeping basic punctuation
            tokens = re.findall(r"[\w']+|[.!?]", text)
            return [token.lower() for token in tokens]
        else:
            # Character level - keep everything
            return list(text)

def train(self, text):
        """Train the Markov model on the provided text"""
        tokens = self.preprocess_text(text)

        if len(tokens) < self.order:
            raise ValueError(f"Text is too short for order {self.order}")

        # Build the model
        for i in range(len(tokens) - self.order):
            # Current state is a tuple of order tokens
            state = tuple(tokens[i:i + self.order])
            next_token = tokens[i + self.order]

            # Add to transition counts
            self.model[state][next_token] += 1

            # Track possible starting states (those that start sentences)
            if i == 0 or tokens[i-1] in '.!?':
                self.start_tokens.append(state)

def generate(self, length=100, start_with=None):
        """Generate text of specified length"""
        if not self.model:
            raise ValueError("Model must be trained before generation")

        # Choose starting state
        if start_with:
            # Use provided starting text
            start_tokens = self.preprocess_text(start_with)
            if len(start_tokens) < self.order:
                # Pad if necessary
                start_tokens.extend([''] * (self.order - len(start_tokens)))
            current_state = tuple(start_tokens[:self.order])
        else:
            # Choose random starting state
            current_state = random.choice(self.start_tokens)

        # Initialize result with starting state
        if self.mode == 'word':
            result = list(current_state)
        else:
            result = list(''.join(current_state))

        # Generate tokens
        for _ in range(length - self.order):
            # Get possible next tokens
            if current_state in self.model:
                next_tokens = list(self.model[current_state].keys())
                weights = list(self.model[current_state].values())

                # Choose next token based on probabilities
                next_token = random.choices(next_tokens, weights=weights)[0]

                # Add to result
                result.append(next_token)

                # Update state
                if self.mode == 'word':
                    current_state = tuple(result[-self.order:])
                else:
                    # For character mode, we need to rebuild the state
                    current_text = ''.join(result)
                    current_state = tuple(current_text[-self.order:])
            else:
                # If we hit an unknown state, start fresh
                current_state = random.choice(self.start_tokens)
                if self.mode == 'word':
                    result.extend(current_state)
                else:
                    result.extend(''.join(current_state))

        # Format output
        if self.mode == 'word':
            # Join words with spaces, handle punctuation
            output = ' '.join(result)
            # Fix spacing around punctuation
            output = re.sub(r'\s+([.,!?])', r'\1', output)
            return output
        else:
            return ''.join(result)

def get_probability(self, state, next_token):
        """Get the probability of a token following a given state"""
        if state not in self.model or next_token not in self.model[state]:
            return 0.0

        total = sum(self.model[state].values())
        return self.model[state][next_token] / total

def demo_markov_chain():
    """Demonstrate the Markov Chain text generation"""

    # Sample training texts
    sample_texts = [
        "The quick brown fox jumps over the lazy dog. The dog barked at the fox.",
        "To be or not to be, that is the question. Whether it is nobler in the mind.",
        "It was the best of times, it was the worst of times. It was the age of wisdom.",
        "I have a dream that one day this nation will rise up. I have a dream today."
    ]

def demo_markov_chain():
    """Demonstrate the Markov Chain text generation"""

    # Sample training texts
    sample_texts = [
        "The quick brown fox jumps over the lazy dog. The dog barked at the fox.",
        "To be or not to be, that is the question. Whether it is nobler in the mind.",
        "It was the best of times, it was the worst of times. It was the age of wisdom.",
        "I have a dream that one day this nation will rise up. I have a dream today."
    ]

    print("=== WORD-LEVEL MARKOV CHAIN ===")
    mc_word = MarkovChain(order=2, mode='word')

    # Train on all sample texts
    for text in sample_texts:
        mc_word.train(text)

    # Generate some text
    for i in range(3):
        generated = mc_word.generate(length=15)
        print(f"Example {i+1}: {generated}")

    print("\n=== CHARACTER-LEVEL MARKOV CHAIN ===")
    mc_char = MarkovChain(order=3, mode='char')

    # Train on all sample texts
    for text in sample_texts:
        mc_char.train(text)

    # Generate some text
    for i in range(3):
        generated = mc_char.generate(length=50)
        print(f"Example {i+1}: {generated}")

    # Show some transition probabilities
    print("\n=== TRANSITION PROBABILITIES (WORD-LEVEL) ===")
    state = ('the', 'quick')
    if state in mc_word.model:
        print(f"State: {state}")
        for token, count in mc_word.model[state].most_common(5):
            prob = mc_word.get_probability(state, token)
            print(f"  '{token}': {prob:.3f}")

if __name__ == "__main__":
    demo_markov_chain()